{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aMFvFjcoI_v"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow GAN Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35cp5a7vN9V8"
      },
      "source": [
        "# TF-GAN Tutorial\n",
        "\n",
        "Tutorial authors: joelshor@, westbrook@"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSTQ5Flu7FMP"
      },
      "source": [
        "## Colab Prelims\n",
        "\n",
        "\n",
        "### Steps to run this notebook\n",
        "\n",
        "This notebook should be run in Colaboratory. If you are viewing this from GitHub, follow the GitHub instructions. If you are viewing this from Colaboratory, you should skip to the Colaboratory instructions.\n",
        "\n",
        "#### Steps from GitHub\n",
        "\n",
        "1. Navigate your web brower to the main Colaboratory website: https://colab.research.google.com.\n",
        "1. Click the `GitHub` tab.\n",
        "1. In the field marked `Enter a GitHub URL or search by organization or user`, put in the URL of this notebook in GitHub and click the magnifying glass icon next to it.\n",
        "1. Run the notebook in colaboratory by following the instructions below.\n",
        "\n",
        "#### Steps from Colaboratory\n",
        "\n",
        "This colab will run much faster on GPU. To use a Google Cloud\n",
        "GPU:\n",
        "\n",
        "1. Go to `Runtime > Change runtime type`.\n",
        "1. Click `Hardware accelerator`.\n",
        "1. Select `GPU` and click `Save`.\n",
        "1. Click `Connect` in the upper right corner and select `Connect to hosted runtime`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "83-azWpoYsDg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-ganNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "     ---------------------------------------- 0.0/367.1 kB ? eta -:--:--\n",
            "     --- ------------------------------------ 30.7/367.1 kB ? eta -:--:--\n",
            "     --------- ----------------------------- 92.2/367.1 kB 1.1 MB/s eta 0:00:01\n",
            "     --------------- ---------------------- 153.6/367.1 kB 1.1 MB/s eta 0:00:01\n",
            "     ---------------------------- --------- 276.5/367.1 kB 1.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- 367.1/367.1 kB 1.3 MB/s eta 0:00:00\n",
            "Collecting tensorflow-hub>=0.2 (from tensorflow-gan)\n",
            "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-probability>=0.7 (from tensorflow-gan)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (3.19.6)\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.2->tensorflow-gan)\n",
            "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: absl-py in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (2.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (1.16.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (5.1.1)\n",
            "Collecting cloudpickle>=1.3 (from tensorflow-probability>=0.7->tensorflow-gan)\n",
            "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: gast>=0.3.2 in c:\\users\\javie\\documents\\proyectos\\python-deep-learning\\.venv\\lib\\site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.4.0)\n",
            "Collecting dm-tree (from tensorflow-probability>=0.7->tensorflow-gan)\n",
            "  Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
            "     ---------------------------------------- 0.0/101.5 kB ? eta -:--:--\n",
            "     -------------------------------------- 101.5/101.5 kB 5.7 MB/s eta 0:00:00\n",
            "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
            "Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.1/6.9 MB 2.6 MB/s eta 0:00:03\n",
            "    --------------------------------------- 0.2/6.9 MB 2.5 MB/s eta 0:00:03\n",
            "   - -------------------------------------- 0.3/6.9 MB 1.8 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 0.3/6.9 MB 1.9 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.4/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.5/6.9 MB 2.0 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.7/6.9 MB 2.0 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 0.8/6.9 MB 2.1 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 0.9/6.9 MB 1.9 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.0/6.9 MB 1.9 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.0/6.9 MB 1.9 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.0/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.1/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.2/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.3/6.9 MB 1.8 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 1.4/6.9 MB 1.8 MB/s eta 0:00:04\n",
            "   -------- ------------------------------- 1.4/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 1.6/6.9 MB 1.7 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 1.6/6.9 MB 1.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.8/6.9 MB 1.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.9/6.9 MB 1.8 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 2.1/6.9 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 2.2/6.9 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 2.4/6.9 MB 2.0 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.4/6.9 MB 2.0 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 2.5/6.9 MB 2.0 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 2.7/6.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.8/6.9 MB 2.0 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.9/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 2.9/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 3.0/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 3.0/6.9 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 3.2/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 3.3/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 3.4/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 3.5/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 3.6/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 3.7/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 3.9/6.9 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 4.1/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 4.1/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 4.2/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 4.2/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 4.4/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 4.7/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 4.7/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 4.8/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 4.8/6.9 MB 2.1 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 4.9/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 5.1/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 5.1/6.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 5.2/6.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 5.2/6.9 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 5.4/6.9 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 5.6/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 5.8/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 6.1/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 6.1/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 6.2/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 6.3/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 6.4/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.6/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.6/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.6/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.8/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  6.8/6.9 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.9/6.9 MB 2.1 MB/s eta 0:00:00\n",
            "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.2/1.7 MB 10.9 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.4/1.7 MB 5.8 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.5/1.7 MB 5.0 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 0.8/1.7 MB 4.6 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 0.8/1.7 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.0/1.7 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 1.4/1.7 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.6/1.7 MB 4.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 3.9 MB/s eta 0:00:00\n",
            "Installing collected packages: dm-tree, tf-keras, cloudpickle, tensorflow-probability, tensorflow-hub, tensorflow-gan\n",
            "Successfully installed cloudpickle-3.0.0 dm-tree-0.1.8 tensorflow-gan-2.1.0 tensorflow-hub-0.16.1 tensorflow-probability-0.23.0 tf-keras-2.15.0\n",
            "\n",
            "\n",
            "Failed to import TensorFlow Probability. To use TF-GAN, please install the most recent version of TensorFlow Probability, by following instructions at https://www.tensorflow.org/probability/install.\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.15; Detected an installation of version 2.10.0. Please upgrade TensorFlow to proceed.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall tensorflow-gan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_gan\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfgan\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_gan\\__init__.py:109\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis version of TF-GAN requires TensorFlow Probability \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[38;5;124m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade TensorFlow to proceed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    104\u001b[0m             required\u001b[38;5;241m=\u001b[39mrequired_tensorflow_probability_version,\n\u001b[0;32m    105\u001b[0m             present\u001b[38;5;241m=\u001b[39mtfp\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[0;32m    108\u001b[0m _ensure_tf_install()\n\u001b[1;32m--> 109\u001b[0m \u001b[43m_ensure_tfp_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_gan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Cleanup symbols to avoid polluting namespace.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_gan\\__init__.py:81\u001b[0m, in \u001b[0;36m_ensure_tfp_install\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Attempt to import tensorflow, and ensure its version is sufficient.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m  ImportError: if either tensorflow_probability is not importable or its\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m  version is inadequate.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfp\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;66;03m# Print more informative error message, then reraise.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to import TensorFlow Probability. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use TF-GAN, please install the most recent version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow Probability, by following instructions at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/probability/install.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\__init__.py:20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m substrates\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\substrates\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Probability Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"TensorFlow Probability alternative substrates.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_util\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_loader  \u001b[38;5;66;03m# pylint: disable=g-direct-tensorflow-import\u001b[39;00m\n\u001b[0;32m     21\u001b[0m jax \u001b[38;5;241m=\u001b[39m lazy_loader\u001b[38;5;241m.\u001b[39mLazyLoader(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m(),\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow_probability.substrates.jax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:138\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[0;32m    136\u001b[0m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m all_util\u001b[38;5;241m.\u001b[39mremove_undocumented(\u001b[38;5;18m__name__\u001b[39m, _lazy_load \u001b[38;5;241m+\u001b[39m _maybe_nonlazy_load)\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:57\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 57\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:37\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access):\n\u001b[1;32m---> 37\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_first_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Javie\\Documents\\Proyectos\\Python-Deep-Learning\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:59\u001b[0m, in \u001b[0;36m_validate_tf_environment\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#   required_tensorflow_version = '1.15'  # Needed internally -- DisableOnExport\u001b[39;00m\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (distutils\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mLooseVersion(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m\n\u001b[0;32m     58\u001b[0m       distutils\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mLooseVersion(required_tensorflow_version)):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis version of TensorFlow Probability requires TensorFlow \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[38;5;124m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease upgrade TensorFlow to proceed.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     63\u001b[0m             required\u001b[38;5;241m=\u001b[39mrequired_tensorflow_version,\n\u001b[0;32m     64\u001b[0m             present\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (package \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcmc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m     67\u001b[0m       tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mtensor_float_32_execution_enabled()):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Must import here, because symbols get pruned to __all__.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.15; Detected an installation of version 2.10.0. Please upgrade TensorFlow to proceed."
          ]
        }
      ],
      "source": [
        "# Check that imports for the rest of the file work.\n",
        "import tensorflow.compat.v1 as tf\n",
        "%pip install tensorflow-gan\n",
        "import tensorflow_gan as tfgan\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Allow matplotlib images to render immediately.\n",
        "%matplotlib inline\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)  # Disable noisy outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2xrX4F-OEL7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This colab will walk you through the basics of using [TF-GAN](https://github.com/tensorflow/gan) to define, train, and evaluate Generative Adversarial Networks (GANs). We describe the library's core features as well as some extra features. This colab assumes a familiarity with TensorFlow's Python API. For more on TensorFlow, please see [TensorFlow tutorials](https://www.tensorflow.org/tutorials/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMljl0ZwONgi"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "In this Colab, you will learn how to:\n",
        "*   Use TF-GAN Estimators to quickly train a GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI8zy5Bz65pa"
      },
      "source": [
        "## Unconditional MNIST with GANEstimator\n",
        "\n",
        "This exercise uses TF-GAN's GANEstimator and the MNIST dataset to create a GAN for generating fake handwritten digits.\n",
        "\n",
        "### MNIST\n",
        "\n",
        "The [MNIST dataset](https://wikipedia.org/wiki/MNIST_database) contains tens of thousands of images of handwritten digits. We'll use these images to train a GAN to generate fake images of handwritten digits. This task is small enough that you'll be able to train the GAN in a matter of minutes.\n",
        "\n",
        "### GANEstimator\n",
        "\n",
        "TensorFlow's Estimator API that makes it easy to train models. TF-GAN offers `GANEstimator`, an Estimator for training GANs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxrYrU887Mns"
      },
      "source": [
        "### Input Pipeline\n",
        "\n",
        "We set up our input pipeline by defining an `input_fn`. in the \"Train and Eval Loop\" section below we pass this function to our GANEstimator's `train` method to initiate training.  The `input_fn`:\n",
        "\n",
        "1.  Generates the random inputs for the generator.\n",
        "2.  Uses `tensorflow_datasets` to retrieve the MNIST data.\n",
        "3.  Uses the tf.data API to format the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs8kdV0w7Rtq"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "def input_fn(mode, params):\n",
        "  assert 'batch_size' in params\n",
        "  assert 'noise_dims' in params\n",
        "  bs = params['batch_size']\n",
        "  nd = params['noise_dims']\n",
        "  split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'test'\n",
        "  shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "  noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\n",
        "              .map(lambda _: tf.random.normal([bs, nd])))\n",
        "\n",
        "  if just_noise:\n",
        "    return noise_ds\n",
        "\n",
        "  def _preprocess(element):\n",
        "    # Map [0, 255] to [-1, 1].\n",
        "    images = (tf.cast(element['image'], tf.float32) - 127.5) / 127.5\n",
        "    return images\n",
        "\n",
        "  images_ds = (tfds.load('mnist:3.*.*', split=split)\n",
        "               .map(_preprocess)\n",
        "               .cache()\n",
        "               .repeat())\n",
        "  if shuffle:\n",
        "    images_ds = images_ds.shuffle(\n",
        "        buffer_size=10000, reshuffle_each_iteration=True)\n",
        "  images_ds = (images_ds.batch(bs, drop_remainder=True)\n",
        "               .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "  return tf.data.Dataset.zip((noise_ds, images_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6aboJBr8Rig"
      },
      "source": [
        "Download the data and sanity check the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEhgLuGo8OGc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_gan as tfgan\n",
        "import numpy as np\n",
        "\n",
        "params = {'batch_size': 100, 'noise_dims':64}\n",
        "with tf.Graph().as_default():\n",
        "  ds = input_fn(tf.estimator.ModeKeys.TRAIN, params)\n",
        "  numpy_imgs = next(iter(tfds.as_numpy(ds)))[1]\n",
        "img_grid = tfgan.eval.python_image_grid(numpy_imgs, grid_shape=(10, 10))\n",
        "plt.axis('off')\n",
        "plt.imshow(np.squeeze(img_grid))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sAetutZ9t93"
      },
      "source": [
        "### Neural Network Architecture\n",
        "\n",
        "To build our GAN we need two separate networks:\n",
        "\n",
        "*  A generator that takes input noise and outputs generated MNIST digits\n",
        "*  A discriminator that takes images and outputs a probability of being real or fake\n",
        "\n",
        "We define functions that build these networks. In the GANEstimator section below we pass the builder functions to the `GANEstimator` constructor. `GANEstimator` handles hooking the generator and discriminator together into the GAN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ9n-jw_MG6C"
      },
      "outputs": [],
      "source": [
        "def _dense(inputs, units, l2_weight):\n",
        "  return tf.layers.dense(\n",
        "      inputs, units, None,\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
        "\n",
        "def _batch_norm(inputs, is_training):\n",
        "  return tf.layers.batch_normalization(\n",
        "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\n",
        "\n",
        "def _deconv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
        "  return tf.layers.conv2d_transpose(\n",
        "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride],\n",
        "      activation=tf.nn.relu, padding='same',\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
        "\n",
        "def _conv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
        "  return tf.layers.conv2d(\n",
        "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride],\n",
        "      activation=None, padding='same',\n",
        "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
        "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHkpn6ks90_R"
      },
      "outputs": [],
      "source": [
        "def unconditional_generator(noise, mode, weight_decay=2.5e-5):\n",
        "  \"\"\"Generator to produce unconditional MNIST images.\"\"\"\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  net = _dense(noise, 1024, weight_decay)\n",
        "  net = _batch_norm(net, is_training)\n",
        "  net = tf.nn.relu(net)\n",
        "\n",
        "  net = _dense(net, 7 * 7 * 256, weight_decay)\n",
        "  net = _batch_norm(net, is_training)\n",
        "  net = tf.nn.relu(net)\n",
        "\n",
        "  net = tf.reshape(net, [-1, 7, 7, 256])\n",
        "  net = _deconv2d(net, 64, 4, 2, weight_decay)\n",
        "  net = _deconv2d(net, 64, 4, 2, weight_decay)\n",
        "  # Make sure that generator output is in the same range as `inputs`\n",
        "  # ie [-1, 1].\n",
        "  net = _conv2d(net, 1, 4, 1, 0.0)\n",
        "  net = tf.tanh(net)\n",
        "\n",
        "  return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-ZqQ4_thIrP"
      },
      "outputs": [],
      "source": [
        "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
        "\n",
        "def unconditional_discriminator(img, unused_conditioning, mode, weight_decay=2.5e-5):\n",
        "  del unused_conditioning\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  net = _conv2d(img, 64, 4, 2, weight_decay)\n",
        "  net = _leaky_relu(net)\n",
        "\n",
        "  net = _conv2d(net, 128, 4, 2, weight_decay)\n",
        "  net = _leaky_relu(net)\n",
        "\n",
        "  net = tf.layers.flatten(net)\n",
        "\n",
        "  net = _dense(net, 1024, weight_decay)\n",
        "  net = _batch_norm(net, is_training)\n",
        "  net = _leaky_relu(net)\n",
        "\n",
        "  net = _dense(net, 1, weight_decay)\n",
        "\n",
        "  return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhTAjxnyPS5e"
      },
      "source": [
        "### Evaluating Generative Models, and evaluating GANs\n",
        "\n",
        "\n",
        "TF-GAN provides some standard methods of evaluating generative models. In this example, we measure:\n",
        "\n",
        "*  Inception Score: called `mnist_score` below.\n",
        "*  Frechet Inception Distance\n",
        "\n",
        "We apply a pre-trained classifier to both the real data and the generated data calculate the *Inception Score*.  The Inception Score is designed to measure both quality and diversity. See [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) by Salimans et al for more information about the Inception Score.\n",
        "\n",
        "*Frechet Inception Distance* measures how close the generated image distribution is to the real image distribution.  See [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500) by Heusel et al for more information about the Frechet Inception distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jF-FW5LPTn6"
      },
      "outputs": [],
      "source": [
        "from tensorflow_gan.examples.mnist import util as eval_util\n",
        "import os\n",
        "\n",
        "def get_eval_metric_ops_fn(gan_model):\n",
        "  real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\n",
        "  gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
        "  real_mnist_score = eval_util.mnist_score(gan_model.real_data)\n",
        "  generated_mnist_score = eval_util.mnist_score(gan_model.generated_data)\n",
        "  frechet_distance = eval_util.mnist_frechet_distance(\n",
        "      gan_model.real_data, gan_model.generated_data)\n",
        "  return {\n",
        "      'real_data_logits': tf.metrics.mean(real_data_logits),\n",
        "      'gen_data_logits': tf.metrics.mean(gen_data_logits),\n",
        "      'real_mnist_score': tf.metrics.mean(real_mnist_score),\n",
        "      'mnist_score': tf.metrics.mean(generated_mnist_score),\n",
        "      'frechet_distance': tf.metrics.mean(frechet_distance),\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxF2-gWHHaej"
      },
      "source": [
        "### GANEstimator\n",
        "\n",
        "The `GANEstimator` assembles and manages the pieces of the whole GAN model. The `GANEstimator` constructor takes the following compoonents for both the generator and discriminator:\n",
        "\n",
        "*  Network builder functions: we defined these in the \"Neural Network Architecture\" section above.\n",
        "*  Loss functions: here we use the wasserstein loss for both.\n",
        "*  Optimizers: here we use `tf.train.AdamOptimizer` for both generator and discriminator training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBd8Vg7lHit8"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 32 #@param\n",
        "noise_dimensions = 64 #@param\n",
        "generator_lr = 0.001 #@param\n",
        "discriminator_lr = 0.0002 #@param\n",
        "\n",
        "def gen_opt():\n",
        "  gstep = tf.train.get_or_create_global_step()\n",
        "  base_lr = generator_lr\n",
        "  # Halve the learning rate at 1000 steps.\n",
        "  lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\n",
        "  return tf.train.AdamOptimizer(lr, 0.5)\n",
        "\n",
        "gan_estimator = tfgan.estimator.GANEstimator(\n",
        "    generator_fn=unconditional_generator,\n",
        "    discriminator_fn=unconditional_discriminator,\n",
        "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
        "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
        "    params={'batch_size': train_batch_size, 'noise_dims': noise_dimensions},\n",
        "    generator_optimizer=gen_opt,\n",
        "    discriminator_optimizer=tf.train.AdamOptimizer(discriminator_lr, 0.5),\n",
        "    get_eval_metric_ops_fn=get_eval_metric_ops_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1uldXfUfstT"
      },
      "source": [
        "### Train and eval loop\n",
        "\n",
        "The `GANEstimator`'s `train()` method initiates GAN training, including the alternating generator and discriminator training phases.\n",
        "\n",
        "The loop in the code below calls `train()` repeatedly in order to periodically display generator output and evaluation results. But note that the code below does not manage the alternation between discriminator and generator: that's all handled automatically by `train()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH6gcvcwHvSn"
      },
      "outputs": [],
      "source": [
        "# Disable noisy output.\n",
        "tf.autograph.set_verbosity(0, False)\n",
        "\n",
        "import time\n",
        "steps_per_eval = 500 #@param\n",
        "max_train_steps = 5000 #@param\n",
        "batches_for_eval_metrics = 100 #@param\n",
        "\n",
        "# Used to track metrics.\n",
        "steps = []\n",
        "real_logits, fake_logits = [], []\n",
        "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\n",
        "\n",
        "cur_step = 0\n",
        "start_time = time.time()\n",
        "while cur_step < max_train_steps:\n",
        "  next_step = min(cur_step + steps_per_eval, max_train_steps)\n",
        "\n",
        "  start = time.time()\n",
        "  gan_estimator.train(input_fn, max_steps=next_step)\n",
        "  steps_taken = next_step - cur_step\n",
        "  time_taken = time.time() - start\n",
        "  print('Time since start: %.2f min' % ((time.time() - start_time) / 60.0))\n",
        "  print('Trained from step %i to %i in %.2f steps / sec' % (\n",
        "      cur_step, next_step, steps_taken / time_taken))\n",
        "  cur_step = next_step\n",
        "\n",
        "  # Calculate some metrics.\n",
        "  metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\n",
        "  steps.append(cur_step)\n",
        "  real_logits.append(metrics['real_data_logits'])\n",
        "  fake_logits.append(metrics['gen_data_logits'])\n",
        "  real_mnist_scores.append(metrics['real_mnist_score'])\n",
        "  mnist_scores.append(metrics['mnist_score'])\n",
        "  frechet_distances.append(metrics['frechet_distance'])\n",
        "  print('Average discriminator output on Real: %.2f  Fake: %.2f' % (\n",
        "      real_logits[-1], fake_logits[-1]))\n",
        "  print('Inception Score: %.2f / %.2f  Frechet Distance: %.2f' % (\n",
        "      mnist_scores[-1], real_mnist_scores[-1], frechet_distances[-1]))\n",
        "\n",
        "  # Vizualize some images.\n",
        "  iterator = gan_estimator.predict(\n",
        "      input_fn, hooks=[tf.train.StopAtStepHook(num_steps=21)])\n",
        "  try:\n",
        "    imgs = np.array([next(iterator) for _ in range(20)])\n",
        "  except StopIteration:\n",
        "    pass\n",
        "  tiled = tfgan.eval.python_image_grid(imgs, grid_shape=(2, 10))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(np.squeeze(tiled))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Plot the metrics vs step.\n",
        "plt.title('MNIST Frechet distance per step')\n",
        "plt.plot(steps, frechet_distances)\n",
        "plt.figure()\n",
        "plt.title('MNIST Score per step')\n",
        "plt.plot(steps, mnist_scores)\n",
        "plt.plot(steps, real_mnist_scores)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy1dsvWuwJeS"
      },
      "source": [
        "### Next steps\n",
        "\n",
        "Try [this colab notebook](https://github.com/tensorflow/gan) to train a GAN on Google's Cloud TPU use TF-GAN.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "TF-GAN Tutorial",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
